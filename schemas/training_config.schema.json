{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "OpenSloth Training Configuration",
  "description": "Complete configuration schema for OpenSloth training with IntelliSense support",
  "type": "object",
  "properties": {
    "opensloth_config": {
      "$defs": {
        "FastModelArgs": {
          "additionalProperties": true,
          "description": "Configuration for Unsloth's FastModel initialization.\n\nDerived from unsloth/models/loader.py: FastModel.from_pretrained",
          "properties": {
            "model_name": {
              "cli_alias": "model",
              "description": "The model name or path to use.",
              "title": "Model Name",
              "type": "string"
            },
            "max_seq_length": {
              "cli_alias": "max-seq-length",
              "default": 4096,
              "description": "Maximum sequence length for the model.",
              "title": "Max Seq Length",
              "type": "integer"
            },
            "load_in_4bit": {
              "cli_alias": "load-in-4bit",
              "default": true,
              "description": "Load the model in 4-bit (QLoRA).",
              "title": "Load In 4Bit",
              "type": "boolean"
            },
            "load_in_8bit": {
              "cli_alias": "load-in-8bit",
              "default": false,
              "description": "Load the model in 8-bit.",
              "title": "Load In 8Bit",
              "type": "boolean"
            },
            "full_finetuning": {
              "cli_alias": "full-finetuning",
              "default": false,
              "description": "Perform full fine-tuning instead of LoRA.",
              "title": "Full Finetuning",
              "type": "boolean"
            },
            "use_gradient_checkpointing": {
              "default": "unsloth",
              "title": "Use Gradient Checkpointing",
              "type": "string"
            },
            "fast_inference": {
              "default": false,
              "title": "Fast Inference",
              "type": "boolean"
            },
            "max_lora_rank": {
              "anyOf": [
                {
                  "type": "integer"
                },
                {
                  "type": "null"
                }
              ],
              "default": null,
              "title": "Max Lora Rank"
            },
            "gpu_memory_utilization": {
              "default": 0.7,
              "title": "Gpu Memory Utilization",
              "type": "number"
            }
          },
          "required": [
            "model_name"
          ],
          "title": "FastModelArgs",
          "type": "object"
        },
        "LoraArgs": {
          "additionalProperties": true,
          "description": "Configuration for LoRA parameters in PEFT.",
          "properties": {
            "finetune_vision_layers": {
              "default": false,
              "title": "Finetune Vision Layers",
              "type": "boolean"
            },
            "finetune_language_layers": {
              "default": true,
              "title": "Finetune Language Layers",
              "type": "boolean"
            },
            "finetune_attention_modules": {
              "default": true,
              "title": "Finetune Attention Modules",
              "type": "boolean"
            },
            "finetune_mlp_modules": {
              "default": true,
              "title": "Finetune Mlp Modules",
              "type": "boolean"
            },
            "r": {
              "cli_alias": "lora-r",
              "default": 8,
              "description": "LoRA rank (`r`).",
              "title": "R",
              "type": "integer"
            },
            "lora_alpha": {
              "cli_alias": "lora-alpha",
              "default": 16,
              "description": "LoRA alpha.",
              "title": "Lora Alpha",
              "type": "integer"
            },
            "lora_dropout": {
              "cli_alias": "lora-dropout",
              "default": 0.0,
              "description": "LoRA dropout.",
              "title": "Lora Dropout",
              "type": "number"
            },
            "bias": {
              "default": "none",
              "title": "Bias",
              "type": "string"
            },
            "random_state": {
              "default": 3407,
              "title": "Random State",
              "type": "integer"
            },
            "target_modules": {
              "cli_alias": "targets",
              "description": "List of target modules for LoRA application",
              "items": {
                "type": "string"
              },
              "title": "Target Modules",
              "type": "array"
            },
            "use_rslora": {
              "cli_alias": "use-rslora",
              "default": false,
              "description": "Use RSLoRA (Rank-Stabilized LoRA).",
              "title": "Use Rslora",
              "type": "boolean"
            }
          },
          "title": "LoraArgs",
          "type": "object"
        }
      },
      "additionalProperties": true,
      "description": "Main configuration class combining all sub-configurations.",
      "properties": {
        "data_cache_path": {
          "description": "Path to cache directory for datasets",
          "title": "Data Cache Path",
          "type": "string"
        },
        "devices": {
          "default": [
            0
          ],
          "description": "List of GPU indices to use",
          "items": {
            "type": "integer"
          },
          "title": "Devices",
          "type": "array"
        },
        "fast_model_args": {
          "$ref": "#/$defs/FastModelArgs"
        },
        "lora_args": {
          "anyOf": [
            {
              "$ref": "#/$defs/LoraArgs"
            },
            {
              "type": "null"
            }
          ],
          "default": null
        },
        "pretrained_lora": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "cli_alias": "pretrained-lora",
          "default": null,
          "description": "Path to pretrained LoRA model for continuous LoRA training",
          "title": "Pretrained Lora"
        },
        "sequence_packing": {
          "default": true,
          "description": "Disable packing of sequences for training",
          "title": "Sequence Packing",
          "type": "boolean"
        },
        "training_type": {
          "const": "sft",
          "default": "sft",
          "description": "Type of training to perform: SFT (Supervised Fine-Tuning)",
          "title": "Training Type",
          "type": "string"
        },
        "log_level": {
          "default": "info",
          "description": "Logging level for the training process",
          "enum": [
            "info",
            "debug"
          ],
          "title": "Log Level",
          "type": "string"
        },
        "filter_overlength_samples": {
          "default": true,
          "description": "If True, filter out dataset samples whose input_ids length exceeds fast_model_args.max_seq_length before training.",
          "title": "Filter Overlength Samples",
          "type": "boolean"
        }
      },
      "required": [
        "data_cache_path",
        "fast_model_args"
      ],
      "title": "OpenSlothConfig",
      "type": "object"
    },
    "training_args": {
      "additionalProperties": true,
      "description": "Configuration for Hugging Face TrainingArguments.",
      "properties": {
        "output_dir": {
          "cli_alias": "output",
          "default": "saves/loras/",
          "description": "Output directory for checkpoints and logs.",
          "title": "Output Dir",
          "type": "string"
        },
        "per_device_train_batch_size": {
          "cli_alias": "bs",
          "default": 8,
          "description": "Batch size per GPU.",
          "title": "Per Device Train Batch Size",
          "type": "integer"
        },
        "learning_rate": {
          "cli_alias": "lr",
          "default": 0.0002,
          "description": "The initial learning rate.",
          "title": "Learning Rate",
          "type": "number"
        },
        "gradient_accumulation_steps": {
          "cli_alias": "grad-accum",
          "default": 4,
          "description": "Number of gradient accumulation steps.",
          "title": "Gradient Accumulation Steps",
          "type": "integer"
        },
        "logging_steps": {
          "default": 1,
          "title": "Logging Steps",
          "type": "integer"
        },
        "num_train_epochs": {
          "cli_alias": "epochs",
          "default": 3,
          "description": "Total number of training epochs.",
          "title": "Num Train Epochs",
          "type": "integer"
        },
        "lr_scheduler_type": {
          "default": "linear",
          "title": "Lr Scheduler Type",
          "type": "string",
          "enum": [
            "linear",
            "cosine",
            "cosine_with_restarts",
            "polynomial",
            "constant",
            "constant_with_warmup",
            "inverse_sqrt",
            "reduce_lr_on_plateau"
          ]
        },
        "warmup_steps": {
          "cli_alias": "warmup",
          "default": 10,
          "description": "Warmup steps for LR scheduler.",
          "title": "Warmup Steps",
          "type": "integer"
        },
        "save_total_limit": {
          "default": 2,
          "title": "Save Total Limit",
          "type": "integer"
        },
        "optim": {
          "default": "adamw_8bit",
          "title": "Optim",
          "type": "string",
          "enum": [
            "adamw_hf",
            "adamw_torch",
            "adamw_torch_fused",
            "adamw_apex_fused",
            "adamw_anyprecision",
            "adafactor",
            "adamw_8bit",
            "adamw_bnb_8bit",
            "lion_8bit",
            "lion_32bit",
            "paged_adamw_8bit",
            "paged_adamw_32bit",
            "paged_lion_8bit",
            "paged_lion_32bit"
          ]
        },
        "weight_decay": {
          "default": 0.01,
          "title": "Weight Decay",
          "type": "number"
        },
        "save_only_model": {
          "default": false,
          "title": "Save Only Model",
          "type": "boolean"
        },
        "resume_from_checkpoint": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "title": "Resume From Checkpoint"
        },
        "seed": {
          "default": 42,
          "title": "Seed",
          "type": "integer"
        },
        "report_to": {
          "default": "tensorboard",
          "enum": [
            "tensorboard",
            "wandb",
            "none"
          ],
          "title": "Report To",
          "type": "string"
        },
        "eval_strategy": {
          "default": "no",
          "title": "Eval Strategy",
          "type": "string"
        },
        "dataset_num_proc": {
          "default": 52,
          "title": "Dataset Num Proc",
          "type": "integer"
        }
      },
      "title": "TrainingArguments",
      "type": "object"
    },
    "dataset_prep_config": {
      "additionalProperties": true,
      "description": "Configuration for dataset preparation.\n\nThis mirrors the arguments used by the os-data CLI but provides\na clean, typed interface for programmatic usage and auto-generation.",
      "properties": {
        "tokenizer_name": {
          "cli_alias": "model",
          "description": "Tokenizer or model identifier/path",
          "title": "Tokenizer Name",
          "type": "string"
        },
        "chat_template": {
          "default": "chatml",
          "description": "Chat template name to apply",
          "title": "Chat Template",
          "type": "string",
          "enum": [
            "chatml",
            "qwen-2.5",
            "llama-3.1",
            "llama-3.2",
            "gemma",
            "mistral",
            "phi-3",
            "zephyr",
            "vicuna",
            "alpaca"
          ]
        },
        "dataset_name": {
          "cli_alias": "input",
          "description": "HF dataset 'repo' or path to a local JSON/JSONL file.",
          "title": "Dataset Name",
          "type": "string",
          "examples": [
            "mlabonne/FineTome-100k",
            "HuggingFaceH4/ultrachat_200k",
            "./local_dataset.json",
            "microsoft/orca-math-word-problems-200k"
          ]
        },
        "split": {
          "default": "train",
          "description": "Dataset split (for HF datasets)",
          "title": "Split",
          "type": "string"
        },
        "num_samples": {
          "cli_alias": "samples",
          "default": -1,
          "description": "Number of samples to process (-1 for all)",
          "title": "Num Samples",
          "type": "integer"
        },
        "num_proc": {
          "cli_alias": "workers",
          "default": 8,
          "description": "Workers for dataset map/tokenization",
          "title": "Num Proc",
          "type": "integer"
        },
        "gpus": {
          "default": 1,
          "description": "Number of GPU shards to create for the dataset.",
          "title": "Gpus",
          "type": "integer"
        },
        "output_dir": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "cli_alias": "data-output",
          "default": null,
          "description": "Output directory for processed data.",
          "title": "Output Dir"
        },
        "train_on_target_only": {
          "default": true,
          "description": "If True, mask non-assistant tokens (response-only training).",
          "title": "Train On Target Only",
          "type": "boolean"
        },
        "instruction_part": {
          "default": "<|im_start|>user\n",
          "description": "Marker that begins a user/instruction turn",
          "title": "Instruction Part",
          "type": "string"
        },
        "response_part": {
          "default": "<|im_start|>assistant\n",
          "description": "Marker that begins an assistant/response turn",
          "title": "Response Part",
          "type": "string"
        },
        "max_seq_length": {
          "cli_alias": "max-seq-length",
          "default": 4096,
          "description": "Maximum sequence length for tokenization.",
          "title": "Max Seq Length",
          "type": "integer"
        },
        "training_type": {
          "default": "sft",
          "description": "The training method (only sft is supported).",
          "hidden": true,
          "title": "Training Type",
          "type": "string"
        },
        "debug": {
          "default": 0,
          "description": "If >0, enable debug mode and dump samples",
          "title": "Debug",
          "type": "integer"
        },
        "hf_token": {
          "anyOf": [
            {
              "type": "string"
            },
            {
              "type": "null"
            }
          ],
          "default": null,
          "description": "Hugging Face token for accessing gated models/datasets",
          "title": "Hf Token"
        }
      },
      "required": [
        "tokenizer_name",
        "dataset_name"
      ],
      "title": "DatasetPrepConfig",
      "type": "object"
    }
  },
  "required": [
    "opensloth_config"
  ],
  "additionalProperties": true
}