{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a062a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing training on GPU 0\n",
      "üìä Generating 10000 training examples...\n",
      "üìä Generating 20 training examples...\n",
      "üåç World size: 1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "OpenSloth - Simple Multi-GPU Training with torchrun\n",
    "\n",
    "This is the new simplified approach for multi-GPU training using PyTorch's built-in\n",
    "Distributed Data Parallel (DDP) via torchrun.\n",
    "\n",
    "Usage:\n",
    "    # Single GPU\n",
    "    python train_scripts/train_ddp.py\n",
    "    \n",
    "    # Multi-GPU (recommended)\n",
    "    torchrun --nproc_per_node=2 train_scripts/train_ddp.py\n",
    "    torchrun --nproc_per_node=4 train_scripts/train_ddp.py\n",
    "    \n",
    "Key benefits:\n",
    "- No complex configuration files needed\n",
    "- Uses standard PyTorch DDP (torchrun)\n",
    "- Works with any Unsloth model\n",
    "- Simple and clean codebase\n",
    "- Automatic GPU detection and setup\n",
    "- Efficient sequence packing with 4D masked attention for better GPU utilization\n",
    "\n",
    "Requirements:\n",
    "- unsloth\n",
    "- transformers (latest version supporting 4D masks)\n",
    "- trl\n",
    "- datasets\n",
    "- flash-attn (pip install flash-attn --no-build-isolation)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTConfig, SFTTrainer # type: ignore\n",
    "\n",
    "# Import our simple DDP patch for Unsloth compatibility\n",
    "from opensloth.patching.ddp_patch import ddp_patch\n",
    "ddp_patch()\n",
    "\n",
    "# -----------------------------\n",
    "# Model Configuration\n",
    "# -----------------------------\n",
    "max_seq_length = 8000  # Increased sequence length for better packing efficiency (model supports up to 32768)\n",
    "lora_rank = 8          # LoRA rank for efficient training\n",
    "\n",
    "# Get local rank for DDP - torchrun sets this automatically\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "print(f\"üöÄ Initializing training on GPU {local_rank}\")\n",
    "\n",
    "# IMPORTANT: For 4-bit models under DDP, you MUST load on the correct GPU per rank\n",
    "# This ensures each process loads the model on its assigned GPU\n",
    "if 'model' not in locals():\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen3-0.6B\",  # Using small model for quick testing\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True,                # Memory efficient 4-bit loading\n",
    "        attn_implementation=\"flash_attention_2\",  # Required for 4D masked packing to prevent cross-contamination\n",
    "        device_map={\"\": local_rank},      # Each process gets its own GPU\n",
    "    )\n",
    "\n",
    "    # Apply LoRA adapter for efficient fine-tuning\n",
    "    # WARNING: Do NOT call .to(device) afterwards for 4-bit models - this will break DDP!\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,                                    # LoRA rank\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention layers\n",
    "        lora_alpha=lora_rank * 2,                      # LoRA alpha (typically 2x rank)\n",
    "        use_gradient_checkpointing=\"unsloth\",           # Memory efficient training\n",
    "        random_state=42,                               # Reproducible results\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Generation\n",
    "# -----------------------------\n",
    "\n",
    "def get_random_realistic_fake_data(n: int = 100, seed: int = 3407):\n",
    "    \"\"\"\n",
    "    Generate a realistic but fake dataset for training demonstration.\n",
    "    \n",
    "    This creates a mix of math problems and trivia questions with their solutions,\n",
    "    formatted as conversational data that works well with chat models.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of examples to generate\n",
    "        seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: Hugging Face dataset ready for training\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    print(f\"üìä Generating {n} training examples...\")\n",
    "\n",
    "    # Question templates for variety\n",
    "    math_templates = [\n",
    "        (\"What is {a} + {b}?\", lambda a, b: f\"The answer is {a+b}.\"),\n",
    "        (\"What is {a} - {b}?\", lambda a, b: f\"The answer is {a-b}.\"),\n",
    "        (\"Multiply {a} and {b}.\", lambda a, b: f\"The result is {a*b}.\"),\n",
    "    ]\n",
    "\n",
    "    trivia_templates = [\n",
    "        (\"What is the capital of {country}?\", lambda country, capital: f\"The capital of {country} is {capital}.\"),\n",
    "        (\"Who wrote {work}?\", lambda work, author: f\"{work} was written by {author}.\"),\n",
    "        (\"In which year did {event} happen?\", lambda event, year: f\"{event} happened in {year}.\"),\n",
    "    ]\n",
    "\n",
    "    # Knowledge base for trivia questions\n",
    "    countries = {\"France\": \"Paris\", \"Japan\": \"Tokyo\", \"Germany\": \"Berlin\", \"Italy\": \"Rome\"}\n",
    "    works = {\"Hamlet\": \"William Shakespeare\", \"1984\": \"George Orwell\", \"The Odyssey\": \"Homer\"}\n",
    "    events = {\"WW2 end\": 1945, \"Moon landing\": 1969, \"Fall of Berlin Wall\": 1989}\n",
    "\n",
    "    problems, solutions = [], []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        if random.random() < 0.5:  # 50% math problems\n",
    "            tmpl, answer_fn = random.choice(math_templates)\n",
    "            a, b = random.randint(1, 20), random.randint(1, 20)\n",
    "            problems.append(tmpl.format(a=a, b=b))\n",
    "            solutions.append(answer_fn(a, b))\n",
    "        else:  # 50% trivia questions\n",
    "            tmpl, answer_fn = random.choice(trivia_templates)\n",
    "            if \"capital\" in tmpl:\n",
    "                country, capital = random.choice(list(countries.items()))\n",
    "                problems.append(tmpl.format(country=country))\n",
    "                solutions.append(answer_fn(country, capital))\n",
    "            elif \"wrote\" in tmpl:\n",
    "                work, author = random.choice(list(works.items()))\n",
    "                problems.append(tmpl.format(work=work))\n",
    "                solutions.append(answer_fn(work, author))\n",
    "            else:  # historical events\n",
    "                event, year = random.choice(list(events.items()))\n",
    "                problems.append(tmpl.format(event=event))\n",
    "                solutions.append(answer_fn(event, year))\n",
    "\n",
    "    # Create structured dataset\n",
    "    fake_data = {\"problem\": problems, \"generated_solution\": solutions}\n",
    "    df = pd.DataFrame(fake_data)\n",
    "    \n",
    "    # Format as chat conversations\n",
    "    df[\"Messages\"] = df.apply(\n",
    "        lambda x: [\n",
    "            {\"role\": \"user\", \"content\": x[\"problem\"]},\n",
    "            {\"role\": \"assistant\", \"content\": x[\"generated_solution\"]},\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    # Apply chat template for the model\n",
    "    df[\"text\"] = tokenizer.apply_chat_template(df[\"Messages\"].tolist(), tokenize=False)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = get_random_realistic_fake_data(n=10000, seed=42)\n",
    "val_dataset = get_random_realistic_fake_data(n=20, seed=2024)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Configuration\n",
    "# -----------------------------\n",
    "\n",
    "# Auto-detect world size (number of GPUs) - torchrun sets this automatically\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "print(f\"üåç World size: {world_size} GPU(s)\")\n",
    "\n",
    "# Smart gradient accumulation: fewer steps for multi-GPU to maintain same effective batch size\n",
    "grad_accum = 1 if world_size > 1 else 2\n",
    "effective_batch_size = 1 * grad_accum * world_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8dad44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Batch configuration:\n",
      "   - Per device batch size: 1\n",
      "   - Gradient accumulation: 2\n",
      "   - Effective batch size: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d77cb63cb747508d38ca5a85f69a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f567a41357f468da7bbb886b84daf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Starting training with 4D masked sequence packing...\n",
      "üíæ Logs will be saved to: outputs/debug_worldsize1\n",
      "üìà Monitor training: tensorboard --logdir outputs/debug_worldsize1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"üìä Batch configuration:\")\n",
    "print(\"   - Per device batch size: 1\")\n",
    "print(f\"   - Gradient accumulation: {grad_accum}\")\n",
    "print(f\"   - Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, # type: ignore\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,      # Small batch per GPU for memory efficiency\n",
    "        gradient_accumulation_steps=3,  # Accumulate gradients for larger effective batch\n",
    "        num_train_epochs=10,                # Number of epochs to train\n",
    "        learning_rate=2e-4,                 # Learning rate for training\n",
    "        logging_steps=1,                    # Log every step for monitoring\n",
    "        save_strategy=\"no\",                 # Don't save checkpoints for this demo\n",
    "        output_dir=f\"outputs/debug_worldsize{world_size}\",  # Output directory\n",
    "        ddp_find_unused_parameters=False,   # DDP optimization\n",
    "        report_to=\"tensorboard\",            # Use tensorboard for logging\n",
    "        eval_strategy=\"epoch\",              # Evaluate at end of each epoch\n",
    "        dataset_num_proc=4,\n",
    "        # packing=True,\n",
    "    ),  \n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Start Training\n",
    "# -----------------------------\n",
    "print(\"üî• Starting training with 4D masked sequence packing...\")\n",
    "if local_rank == 0:  # Only print from main process\n",
    "    print(f\"üíæ Logs will be saved to: outputs/debug_worldsize{world_size}\")\n",
    "    print(f\"üìà Monitor training: tensorboard --logdir outputs/debug_worldsize{world_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4672a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensloth.patching.utils import patch_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b8923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# 1. Keep the original around\n",
    "if not hasattr(Trainer, \"orig_get_batch_samples\"):\n",
    "    Trainer.orig_get_batch_samples = Trainer.get_batch_samples\n",
    "\n",
    "# 2. Define your wrapper\n",
    "def patched_get_batch_samples(self, epoch_iterator, num_batches, device, *args, **kwargs):\n",
    "    print(\"üì¶ Intercepted get_batch_samples\")  # <-- your debug/hook\n",
    "    # Call the original method\n",
    "    out = Trainer.orig_get_batch_samples(self, epoch_iterator, num_batches, device, *args, **kwargs)\n",
    "    # Optionally mutate/inspect result here\n",
    "    return out\n",
    "\n",
    "# 3. Plug the patched version into Trainer\n",
    "Trainer.get_batch_samples = patched_get_batch_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba91d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 10 | Total steps = 16,670\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 3\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 3 x 1) = 6\n",
      " \"-____-\"     Trainable parameters = 2,293,760 of 598,343,680 (0.38% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Intercepted get_batch_samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='16670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   10/16670 00:03 < 2:11:50, 2.11 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n",
      "üì¶ Intercepted get_batch_samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_rank == \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Only print from main process\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Training completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/.venv/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:325\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/train_scripts/unsloth_compiled_cache/UnslothSFTTrainer.py:973\u001b[39m, in \u001b[36m_UnslothSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:83\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/opensloth/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "if local_rank == 0:  # Only print from main process\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"üéØ Model trained for {trainer.state.epoch} epochs\")\n",
    "    print(f\"üìä Total steps: {trainer.state.global_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
