{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e93a9a2e-020f-4b0a-81ee-2d97bb8d227c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[opensloth.ddp_patch] Before repack: total_tokens=22180 useful_tokens=14099 efficiency=63.57%\n",
      "[opensloth.ddp_patch] DP group boundaries (start,end,len,max):\n",
      "  Batch[0] = [0:0] (size=1, max=495)\n",
      "  Batch[1] = [1:3] (size=3, max=461)\n",
      "  Batch[2] = [4:8] (size=5, max=419)\n",
      "  Batch[3] = [9:13] (size=5, max=384)\n",
      "  Batch[4] = [14:16] (size=3, max=364)\n",
      "  Batch[5] = [17:18] (size=2, max=337)\n",
      "  Batch[6] = [19:23] (size=5, max=314)\n",
      "  Batch[7] = [24:26] (size=3, max=284)\n",
      "  Batch[8] = [27:28] (size=2, max=240)\n",
      "  Batch[9] = [29:32] (size=4, max=221)\n",
      "  Batch[10] = [33:36] (size=4, max=188)\n",
      "  Batch[11] = [37:42] (size=6, max=147)\n",
      "  Batch[12] = [43:47] (size=5, max=119)\n",
      "  Batch[13] = [48:51] (size=4, max=86)\n",
      "  Batch[14] = [52:57] (size=6, max=57)\n",
      "  Batch[15] = [58:63] (size=6, max=27)\n",
      "[opensloth.ddp_patch] After repack: total_tokens=14522 useful_tokens=14099 efficiency=97.09%\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Sequence, Iterable, Tuple\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities and containers\n",
    "# ---------------------------\n",
    "\n",
    "class PackedBatch:\n",
    "    \"\"\"Holds variable-length samples before padding into tensors.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_ids: List[torch.Tensor] = []\n",
    "        self.attention_mask: List[torch.Tensor] = []\n",
    "        self.labels: List[torch.Tensor] = []\n",
    "\n",
    "    def max_length(self) -> int:\n",
    "        if not self.input_ids:\n",
    "            return 0\n",
    "        return max(t.size(0) for t in self.input_ids)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Pad samples into a fixed-size batch dictionary.\"\"\"\n",
    "        if not self.input_ids:\n",
    "            return {\n",
    "                \"input_ids\": torch.empty(0, dtype=torch.long),\n",
    "                \"attention_mask\": torch.empty(0, dtype=torch.long),\n",
    "                \"labels\": torch.empty(0, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "        batch_size = len(self.input_ids)\n",
    "        max_len = self.max_length()\n",
    "\n",
    "        input_ids_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "        attention_mask_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "        labels_tensor = torch.full((batch_size, max_len), -100, dtype=torch.long)\n",
    "\n",
    "        for i, (ids, mask, lbls) in enumerate(zip(self.input_ids, self.attention_mask, self.labels)):\n",
    "            L = ids.size(0)\n",
    "            input_ids_tensor[i, :L] = ids\n",
    "            attention_mask_tensor[i, :L] = mask\n",
    "            labels_tensor[i, :L] = lbls\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_tensor,\n",
    "            \"attention_mask\": attention_mask_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "        }\n",
    "\n",
    "\n",
    "def summarize_batches(batches: Sequence[Dict[str, torch.Tensor]], label: str) -> Tuple[int, int]:\n",
    "    total_tokens = sum(batch[\"input_ids\"].numel() for batch in batches)\n",
    "    useful_tokens = sum((batch[\"labels\"] != -100).sum().item() for batch in batches)\n",
    "    efficiency = (useful_tokens / total_tokens) if total_tokens else 0.0\n",
    "    if label:\n",
    "        print(\n",
    "            f\"[opensloth.ddp_patch] {label}: total_tokens={total_tokens} \"\n",
    "            f\"useful_tokens={useful_tokens} efficiency={efficiency:.2%}\"\n",
    "        )\n",
    "    return total_tokens, useful_tokens\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Sample extraction\n",
    "# ---------------------------\n",
    "\n",
    "def extract_samples(batches: Iterable[Dict[str, torch.Tensor]]) -> List[Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Flatten a list of padded batches into per-sample dicts with true (unpadded) length.\n",
    "    Sorts samples by length descending (longest-first).\n",
    "    \"\"\"\n",
    "    samples: List[Dict[str, torch.Tensor]] = []\n",
    "    for batch in batches:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        lengths = attention_mask.sum(dim=1).tolist()\n",
    "\n",
    "        for row, length in enumerate(lengths):\n",
    "            length = int(length)\n",
    "            samples.append(\n",
    "                {\n",
    "                    \"input_ids\": input_ids[row, :length],\n",
    "                    \"attention_mask\": attention_mask[row, :length],\n",
    "                    \"labels\": labels[row, :length],\n",
    "                    \"length\": length,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Sort by sequence length descending (longest first)\n",
    "    samples.sort(key=lambda item: item[\"length\"], reverse=True)\n",
    "    return samples\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Optimal repacking (DP, contiguous groups)\n",
    "# ---------------------------\n",
    "\n",
    "def _optimal_partition_by_length(lengths: List[int], k: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition a descending-sorted length array into k contiguous, non-empty groups\n",
    "    minimizing sum over groups of (max_length_in_group * group_size).\n",
    "    Returns list of (start_idx, end_idx) inclusive, covering 0..n-1.\n",
    "    DP complexity: O(n * k^2) worst-case (fine for typical batch sizes).\n",
    "    \"\"\"\n",
    "    n = len(lengths)\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be >= 1\")\n",
    "    if n < k:\n",
    "        # It's impossible to have k non-empty groups if we have fewer samples than groups.\n",
    "        # Caller ensures batches come from real dataloaders, so assert loudly here.\n",
    "        raise ValueError(f\"Not enough samples ({n}) to fill {k} non-empty batches\")\n",
    "\n",
    "    INF = 10**18\n",
    "    # dp[t][j]: min cost to partition 0..j into t groups (1-index t)\n",
    "    dp = [[INF] * n for _ in range(k)]\n",
    "    prev = [[-1] * n for _ in range(k)]  # prev split index s for dp[t][j]: last group is s+1..j\n",
    "\n",
    "    # Base: t=1 -> one group: cost is lengths[0] * (j+1)\n",
    "    for j in range(n):\n",
    "        dp[0][j] = lengths[0] * (j + 1)\n",
    "        prev[0][j] = -1  # start at 0\n",
    "\n",
    "    # Fill DP\n",
    "    for t in range(1, k):  # groups 2..k\n",
    "        # We need at least t items to form t groups (each non-empty), so j >= t\n",
    "        for j in range(t, n):\n",
    "            best_cost = INF\n",
    "            best_s = -1\n",
    "            # s is end index of previous partition; last group is s+1..j, so s âˆˆ [t-2 .. j-1]\n",
    "            s_min = t - 2\n",
    "            if s_min < -1:\n",
    "                s_min = -1\n",
    "            for s in range(max(s_min, -1), j):\n",
    "                # cost of previous t groups on 0..s, plus cost of group (s+1..j)\n",
    "                # since lengths sorted desc, max of group (s+1..j) is lengths[s+1]\n",
    "                group_size = j - (s + 1) + 1  # = j - s\n",
    "                cost = (dp[t - 1][s] if s >= 0 else INF) + lengths[s + 1] * group_size\n",
    "                if cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_s = s\n",
    "            dp[t][j] = best_cost\n",
    "            prev[t][j] = best_s\n",
    "\n",
    "    # Reconstruct boundaries\n",
    "    bounds: List[Tuple[int, int]] = []\n",
    "    t = k - 1\n",
    "    j = n - 1\n",
    "    while t >= 0:\n",
    "        s = prev[t][j]\n",
    "        start = s + 1 if t > 0 else 0\n",
    "        bounds.append((start, j))\n",
    "        j = s\n",
    "        t -= 1\n",
    "    bounds.reverse()\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def repack_batches(\n",
    "    batches: Sequence[Dict[str, torch.Tensor]],\n",
    "    verbose: bool = False\n",
    ") -> List[Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Optimal repacking (given fixed number of output batches and non-empty constraint).\n",
    "    1) Extract and sort samples by length (desc).\n",
    "    2) Use DP to partition into K contiguous groups minimizing sum(max_len * group_size).\n",
    "    3) Assign groups to K new PackedBatch containers.\n",
    "    \"\"\"\n",
    "    K = len(batches)\n",
    "    samples = extract_samples(batches)\n",
    "    lengths = [s[\"length\"] for s in samples]\n",
    "    bounds = _optimal_partition_by_length(lengths, K)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[opensloth.ddp_patch] DP group boundaries (start,end,len,max):\")\n",
    "    packed_batches = [PackedBatch() for _ in range(K)]\n",
    "\n",
    "    for bi, (lo, hi) in enumerate(bounds):\n",
    "        # max len inside the group is at 'lo' (descending order)\n",
    "        if verbose:\n",
    "            group_max = samples[lo][\"length\"]\n",
    "            print(f\"  Batch[{bi}] = [{lo}:{hi}] \"\n",
    "                  f\"(size={hi-lo+1}, max={group_max})\")\n",
    "        target = packed_batches[bi]\n",
    "        for idx in range(lo, hi + 1):\n",
    "            s = samples[idx]\n",
    "            target.input_ids.append(s[\"input_ids\"])\n",
    "            target.attention_mask.append(s[\"attention_mask\"])\n",
    "            target.labels.append(s[\"labels\"])\n",
    "\n",
    "    # Convert to padded tensors\n",
    "    return [pb.to_dict() for pb in packed_batches]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Mock data for quick testing\n",
    "# ---------------------------\n",
    "\n",
    "def make_mock_batches(\n",
    "    num_batches: int,\n",
    "    batch_size: int,\n",
    "    max_len: int,\n",
    "    seed: int = 42\n",
    ") -> List[Dict[str, torch.Tensor]]:\n",
    "    rng = random.Random(seed)\n",
    "    batches = []\n",
    "    for _ in range(num_batches):\n",
    "        lengths = [rng.randint(5, max_len) for _ in range(batch_size)]\n",
    "        Lmax = max(lengths)\n",
    "        input_ids = torch.zeros((batch_size, Lmax), dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, Lmax), dtype=torch.long)\n",
    "        labels = torch.full((batch_size, Lmax), -100, dtype=torch.long)\n",
    "\n",
    "        for i, L in enumerate(lengths):\n",
    "            input_ids[i, :L] = torch.arange(L) % 1000\n",
    "            attention_mask[i, :L] = 1\n",
    "            labels[i, :L] = torch.arange(L) % 1000\n",
    "\n",
    "        batches.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels})\n",
    "    return batches\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Demo\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create toy input: same number of output batches must be kept and none empty\n",
    "    orig_batches = make_mock_batches(num_batches=16, batch_size=4, max_len=500)\n",
    "    summarize_batches(orig_batches, \"Before repack\")\n",
    "\n",
    "    # Optimal (DP) repack with the same number of batches\n",
    "    new_batches = repack_batches(orig_batches, verbose=True)\n",
    "    summarize_batches(new_batches, \"After repack\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f4802f4-7194-43fc-8096-f8432d0643b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 916,  208, 2255, 2008, 1831, 1146,  843, 4467,  716, 4837, 3457,  264,\n",
       "         248,  771, 1794, 1908, 4139, 4931,  221, 4597, 1631, 4464, 3437, 1808,\n",
       "        3680, 4827, 2280,   57, 1310, 3463, 2789, 2278, 1276, 1766, 2759,  841,\n",
       "         763, 3113,  796, 2942, 2819, 4945, 2168,  359, 3764, 4392, 1025, 3101,\n",
       "         649, 4522])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c432e3b-d1d9-4774-9f76-a68bd93c4bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
