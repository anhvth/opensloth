#!/bin/bash

# OpenSloth GRPO Tutorial - Step 2: End-to-End GRPO Training
# This script prepares the GRPO dataset and runs preference optimization in one command

set -e  # Exit on any error

echo "üöÄ Starting End-to-End GRPO Training"
echo "===================================="

# Ensure we're in the correct directory
if [ ! -f "pyproject.toml" ] || [ ! -d "src/opensloth" ]; then
    echo "‚ùå Error: This script must be run from the root of the opensloth project"
    echo "Please cd to the opensloth directory and run:"
    echo "bash grpo_tutorial/scripts/02_grpo_end_to_end.sh"
    exit 1
fi

# Check if SFT training was completed
if [ ! -d "outputs/sft_qwen_reasoning_model" ]; then
    echo "‚ùå Error: SFT model not found at outputs/sft_qwen_reasoning_model"
    echo "Please run: bash grpo_tutorial/scripts/01_sft_end_to_end.sh"
    exit 1
fi

echo "üìä Step 1: Verifying SFT model..."
echo "‚úÖ SFT model found at outputs/sft_qwen_reasoning_model"

# Extract base model path from adapter config
BASE_MODEL=$(python -c "import json; config = json.load(open('outputs/sft_qwen_reasoning_model/adapter_config.json')); print(config['base_model_name_or_path'])")
echo "üîç Detected base model: $BASE_MODEL"

# Verify base model exists
if [ ! -d "$BASE_MODEL" ]; then
    echo "‚ùå Error: Base model not found at $BASE_MODEL"
    echo "Please ensure the base model is available"
    exit 1
fi

# Step 2: Run end-to-end GRPO training with os-train
echo "üéØ Step 2: Starting end-to-end GRPO training..."
echo "Goal: Improving mathematical reasoning quality using preference optimization"

# Detect GPU count
if [ -z "$GPU_COUNT" ]; then
    GPU_COUNT=$(python -c 'import torch,os;print(torch.cuda.device_count() if torch.cuda.is_available() else 1)')
fi
echo "üîç Detected GPUs: $GPU_COUNT"

# Set sample count for tutorial (can be overridden)
GRPO_SAMPLES=${GRPO_SAMPLES:-4000}

# Create output directory
mkdir -p outputs/grpo_final_model

echo "üîÑ Running os-train grpo (end-to-end: data prep + training)..."
echo "üì• Dataset: open-r1/DAPO-Math-17k-Processed (HuggingFace)"
echo "üîó Using base model: $BASE_MODEL"
echo "üß© Loading pretrained LoRA from: outputs/sft_qwen_reasoning_model"
echo "üöÄ This enables vLLM fast inference for GRPO training"

# Use tmux for multi-GPU if more than 1 GPU is available
TMUX_FLAG=""
if [ "$GPU_COUNT" -gt 1 ]; then
    TMUX_FLAG="--tmux"
    echo "üñ•Ô∏è  Multi-GPU detected: Using tmux for training"
fi

# Run os-train grpo with separated base model and pretrained LoRA
os-train grpo open-r1/DAPO-Math-17k-Processed \
    --model "$BASE_MODEL" \
    --pretrained-lora "outputs/sft_qwen_reasoning_model" \
    --output outputs/grpo_final_model \
    --samples ${GRPO_SAMPLES} \
    --gpus ${GPU_COUNT} \
    --max-seq-length 4096 \
    --epochs 1 \
    --batch-size 64 \
    --lr 5e-5 \
    ${TMUX_FLAG}

# Verify the training completed successfully
if [ ! -d "outputs/grpo_final_model" ] || [ -z "$(ls -A outputs/grpo_final_model)" ]; then
    echo "‚ùå Error: GRPO training failed - output directory is empty"
    exit 1
fi

# Check for adapter files or model files
if [ ! -f "outputs/grpo_final_model/adapter_model.safetensors" ] && 
   [ ! -f "outputs/grpo_final_model/pytorch_model.bin" ] &&
   [ ! -f "outputs/grpo_final_model/model.safetensors" ]; then
    echo "‚ùå Error: GRPO training failed - no model files found"
    exit 1
fi

echo "üéâ End-to-end GRPO training completed successfully!"
echo ""
echo "üìä Training Summary:"
echo "  - Base model: $BASE_MODEL"
echo "  - Pretrained LoRA: outputs/sft_qwen_reasoning_model"
echo "  - Training type: Group Relative Policy Optimization (GRPO)"
echo "  - Purpose: Improving mathematical reasoning accuracy"
echo "  - Dataset: open-r1/DAPO-Math-17k-Processed"
echo "  - Dataset samples: ${GRPO_SAMPLES}"
echo "  - Epochs: 1"
echo "  - GPUs: ${GPU_COUNT}"
echo "  - Output: outputs/grpo_final_model/"
echo "  - vLLM enabled: ‚úÖ (for fast inference during training)"
echo ""
echo "üéØ What the model learned:"
echo "  - Improved mathematical reasoning accuracy"
echo "  - Better problem-solving strategies"
echo "  - Enhanced correctness in solutions"
echo "  - Maintained formatting from SFT stage"
echo ""
echo "üí° Note: os-train automatically:"
echo "  - Downloaded and processed the GRPO dataset"
echo "  - Tokenized and sharded data across ${GPU_COUNT} GPU(s)"
echo "  - Ran GRPO training with consistent GPU allocation"
echo "  - Applied built-in math reward functions"
echo ""
echo "‚úÖ Tutorial Complete!"
echo "===================="
echo ""
echo "üéä Congratulations! You have successfully created a math reasoning model using:"
echo "  1. ‚úÖ End-to-end SFT for format learning"
echo "  2. ‚úÖ End-to-end GRPO for reasoning improvement"
echo ""
echo "üìÅ Your final model is ready at: outputs/grpo_final_model/"
echo ""
echo "üöÄ Next Steps:"
echo "  - Test the model: python grpo_tutorial/scripts/test_final_model.py"
echo "  - Use the model for inference (see README.md for examples)"
echo "  - Experiment with different hyperparameters"
echo "  - Try the workflow on your own datasets"
echo "  - Deploy the model in production applications"
echo ""
echo "üìñ For inference examples and more details, see:"
echo "   grpo_tutorial/README.md"
